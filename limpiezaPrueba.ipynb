{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "df = pd.read_csv('pericias_medicas.csv', sep=',',  encoding='utf-8')\n",
    "#convierte en string los datos de la columna text\n",
    "df = df['text'].apply(str)\n",
    "df1 = df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIMPIEZA DE TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de texto\n",
    "# Primera etapa de la limpieza de texto\n",
    "import re, string, unicodedata\n",
    "# Se hace la eliminacion de texto entre corchetes, acentos, signos de puntuacion (excepto . y :), palabras con numeros.\n",
    "# Se eliminan los espacios de sobra\n",
    "# Se eliminan \\r, \\t, \\v, \\f, \\a\n",
    "def limpiarTexto1(txt: str, bert=False, nums=False) -> str:\n",
    "    \"\"\"\n",
    "    Elimina caracteres no deseados\n",
    "    Params:\n",
    "        **txt**:texto a ser limpiado de caracteres no desaeados\n",
    "    \"\"\"\n",
    "    if nums:\n",
    "        txt = re.sub(r'\\d+', ' ', txt)\n",
    "    if not bert:\n",
    "        txt = txt.translate(str.maketrans(\n",
    "            'áéíóúýàèìòùÁÉÍÓÚÀÈÌÒÙÝ', 'aeiouyaeiouAEIOUAEIOUY'))\n",
    "\n",
    "    '''\n",
    "    Eliminamos caracteres especiales: tabulador orizontal(\\t), tabulador vertical(\\v), \n",
    "    retorno de carro(\\r), avance de pagina(\\f), \n",
    "    caracter de retroceso: Marca el límite de una palabra(\\b), \n",
    "    '''\n",
    "    txt = txt.replace('\\r', ' ').replace(\"\\v\", ' ').replace(\n",
    "        \"\\t\", ' ').replace(\"\\f\", ' ').replace(\"\\a\", ' ').replace(\"\\b\", ' ')\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = txt.strip()\n",
    "    return txt\n",
    "df1.apply(str)\n",
    "df1 = df1.apply(limpiarTexto1)\n",
    "#pprint(df1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segunda fase de limpieza\n",
    "# Se eliminan todos los elementos que meten ruido al texto y que no fueron eliminados en la fase de limpieza 1.\n",
    "import re\n",
    "\n",
    "def limpiarTexto2(text):\n",
    "    text = re.sub('^ ',' ',text)\n",
    "    text = re.sub('\\n +\\n','\\n',text)\n",
    "    text = re.sub(' +\\n\\n','\\n',text)\n",
    "    text = re.sub('\\n\\n+','\\n',text)\n",
    "    text = re.sub(' \\n','\\n',text)\n",
    "    text = re.sub('\\d\\n','',text)\n",
    "    text = re.sub('\\x0c','',text)\n",
    "    text = re.sub('\\u200b\\n','',text)\n",
    "    text = re.sub('\\d{3,100}','',text)\n",
    "    text = re.sub('\\d+-\\d+','',text)\n",
    "    text = re.sub('\\x0c','',text)\n",
    "    text = re.sub('[nN]º|[nN][. ]º','',text)\n",
    "    text = re.sub('[º<>/]','',text)\n",
    "    text = re.sub('\\d{3,100}','',text)\n",
    "    text = re.sub('[a-zA-z-.]+@[a-zA-Z]+.com','',text)\n",
    "    return text\n",
    "\n",
    "dfLimpio = df1.apply(limpiarTexto2)\n",
    "#pprint(dfLimpio[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tercera fase de limpieza\n",
    "# Eliminamos las lineas que no son de utilidad para el analisis o que van a afectar los resultados del mismo.\n",
    "# Ejemplo de linea eliminada: las lineas que comienzan con \"Se encuentra contestada en.....\"\n",
    "import re\n",
    "\n",
    "def limpiarTexto3(text):\n",
    "    text = re.sub('[a-z1-9.]+[).-] [s|S]e encuentra contestad[a|o] .+[. \\n]','',text)\n",
    "    text = re.sub('[fF]oja [1-9].+\\n', '', text)\n",
    "    text = re.sub('[pP]regunta[ 0-9]+[)].+\\n|[rR]espuesta[ 0-9]+[)].+\\n','',text)#elimina oraciones comenzadas en preguta/respuesta.\n",
    "    return text\n",
    "\n",
    "dfLimpio = dfLimpio.apply(limpiarTexto3)\n",
    "#pprint(dfLimpio[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busqueda de titulos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminacion de stopwords y lematizacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "#elimino stopwords\n",
    "def remove_stops(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Función que elimina stopwords\n",
    "    Params:\n",
    "        **texto**:texto a ser limpiado de stopwords\n",
    "\n",
    "    \"\"\"\n",
    "    texto = [\n",
    "        i for i in texto.split() if i not in stops\n",
    "    ]\n",
    "    return texto\n",
    "\n",
    "df1 = df1.apply(remove_stops)\n",
    "\n",
    "#print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "documento = df1.apply(str)  \n",
    "# crear la transformación  \n",
    "vectorizer = CountVectorizer() \n",
    "# tokenizar y construir el vocabulario\n",
    "vectorizer.fit(documento) \n",
    "  \n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_) \n",
    "# codificador de documentos\n",
    "vector = vectorizer.transform(documento) \n",
    "\n",
    "# resumir vector codificado\n",
    "#print(\"El documento codificado es:\") \n",
    "#print(vector.shape)\n",
    "#print(type(vector))\n",
    "#print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiarTextoPrueba3(text):\n",
    "    text = re.sub('[a-z1-9.]+[).-] [s|S]e encuentra contestad[a|o] .+[. \\n]','',text)\n",
    "    text = re.sub('[fF]oja [1-9].+\\n', '', text)\n",
    "    text = re.sub('[pP]regunta[ 0-9]+[)].+\\n|[rR]espuesta[ 0-9]+[)].+\\n','',text)#elimina oraciones comenzadas en preguta/respuesta.\n",
    "    return text\n",
    "\n",
    "#pprint(df[1])\n",
    "pprint(limpiarTextoPrueba3(dfLimpio[1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42875111758dffc1ca879dbe67063d4b14d2fb8b4b9c438105dae1201065df52"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
