{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorización de texto\n",
    "### Creación de los dataframe y limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "df = pd.read_csv('titulosEncontrados.csv', sep=',',  encoding='utf-8')\n",
    "#convierte en string los datos de la columna text\n",
    "df1 = df['titulo'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase de limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza de caracteres no deseados\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def general(txt: str, bert=False, nums=False) -> str:\n",
    "    \"\"\"\n",
    "    Elimina caracteres no deseados\n",
    "    Params:\n",
    "        **txt**:texto a ser limpiado de caracteres no desaeados\n",
    "    \"\"\"\n",
    "    if nums:\n",
    "        txt = re.sub(r'\\d+', ' ', txt)\n",
    "    if not bert:\n",
    "        txt = txt.translate(str.maketrans(\n",
    "            'áéíóúýàèìòùÁÉÍÓÚÀÈÌÒÙÝ', 'aeiouyaeiouAEIOUAEIOUY'))\n",
    "        txt = re.sub(r'[^\\w\\s:.)-]', '', txt)\n",
    "\n",
    "    txt = txt.replace('\\r', ' ').replace(\"\\v\", ' ').replace(\n",
    "        \"\\t\", ' ').replace(\"\\f\", ' ').replace(\"\\a\", ' ').replace(\"\\b\", ' ')\n",
    "    txt = re.sub(' +', ' ', txt)\n",
    "    txt = txt.strip()\n",
    "    return txt\n",
    "\n",
    "dfLimpio = df1.apply(general)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpieza de stopwords\n",
    "\n",
    "stops = nltk.corpus.stopwords.words('spanish')\n",
    "# for i in stops:\n",
    "#     stops.append(i.upper())\n",
    "\n",
    "\n",
    "def remove_stops(texto: str) -> str:\n",
    "    \"\"\"\n",
    "    Función que elimina stopwords\n",
    "    Params:\n",
    "        **texto**:texto a ser limpiado de stopwords\n",
    "\n",
    "    \"\"\"\n",
    "    texto = [\n",
    "        i for i in texto.split() if i not in stops\n",
    "    ]\n",
    "    return ' '.join(texto)\n",
    "\n",
    "dfLimpio = dfLimpio.apply(remove_stops)\n",
    "dfLimpio = dfLimpio.apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5390, 25)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=25,\n",
    "                                 min_df=100, stop_words=stops,\n",
    "                                 use_idf=True, ngram_range=(1,5))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(dfLimpio) #fit the vectorizer to dfLimpio\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['actor', 'antecedentes', 'antecedentes autos', 'autos', 'clinica',\n",
       "       'conclusiones', 'consideraciones', 'consideraciones medico',\n",
       "       'consideraciones medico legales', 'contestacion',\n",
       "       'contestacion puntos', 'examen', 'historia', 'historia clinica',\n",
       "       'interes', 'interes medico', 'legal', 'legales', 'medico',\n",
       "       'medico legal', 'medico legales', 'pericia', 'petitorio', 'puntos',\n",
       "       'puntos pericia'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  1.00000000e+00  1.00000000e+00 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00 -2.22044605e-16  1.00000000e+00 ...  1.00000000e+00\n",
      "   1.00000000e+00  8.75989722e-01]\n",
      " [ 1.00000000e+00  1.00000000e+00 -2.22044605e-16 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " ...\n",
      " [ 1.00000000e+00  1.00000000e+00  1.00000000e+00 ...  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  1.00000000e+00  1.00000000e+00 ...  1.00000000e+00\n",
      "   0.00000000e+00  1.00000000e+00]\n",
      " [ 1.00000000e+00  8.75989722e-01  1.00000000e+00 ...  1.00000000e+00\n",
      "   1.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas de tfidf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el bag of words\n",
    "word2count = {}  \n",
    "for data in dfLimpio:  \n",
    "    words = nltk.word_tokenize(data)  \n",
    "    for word in words:  \n",
    "        if word not in word2count.keys():  \n",
    "            word2count[word] = 1\n",
    "        else:  \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular la puntuación tf-idf para cualquier corpus\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import pandas as pd \n",
    "  \n",
    "tfidf = TfidfVectorizer(min_df = 100, max_df = 0.80, ngram_range = (1, 3), max_features=25) \n",
    "features = tfidf.fit_transform(dfLimpio) \n",
    "  \n",
    "dfTfIdf = pd.DataFrame(features.todense(), columns = tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 10\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "km.fit(features)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abor atencion publico ocasiones', 'sesiones'),\n",
       " ('actitud general', 'puntos'),\n",
       " ('actividad laboral', 'existen constancias'),\n",
       " ('actor', 'actualidad existe patologia'),\n",
       " ('actor antecedentes patologias previas', 'otorgo prestaciones tiempo forma'),\n",
       " ('actor debio intervenido', 'actualidad requiere tratamiento cirugia'),\n",
       " ('actor necesita', 'efectivamente trabajador puede'),\n",
       " ('actor padece sintomatologia relatada',\n",
       "  'requiere recalificacion profesional'),\n",
       " ('actor posee algun', 'punto vista psiquiatrico'),\n",
       " ('actor presenta actualidad', 'actor'),\n",
       " ('actor presenta algun grado secuela indole',\n",
       "  'considera necesidad tratamiento'),\n",
       " ('actor presenta momento actual patologia', 'acarrea'),\n",
       " ('actor recibio adecuada atencion medica', 'actor posee algun'),\n",
       " ('actor solicito apoyo psicoterapeutico', 'posee personalidad'),\n",
       " ('actora presenta momento actual patologia',\n",
       "  'patologia guarda relacion accidente'),\n",
       " ('actualidad existe patologica psiquiatrica', 'hector javier galeno'),\n",
       " ('adjuntos', 'antecedentes accidente'),\n",
       " ('afecciones', 'correcto grado'),\n",
       " ('alejandra nilda', 'indica continuar'),\n",
       " ('alvarez', 'neumaticos')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pruebas de collocation - n-grams\n",
    "import nltk\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "bi_gram= nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "Collocation = BigramCollocationFinder.from_words(dfLimpio)\n",
    "\n",
    "Collocation.nbest(bi_gram.pmi, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'Length'),\n",
       " (',', 'dtype'),\n",
       " ('...', '5385'),\n",
       " ('0', 'proemio'),\n",
       " ('1', 'antecedentes'),\n",
       " ('2', 'examen'),\n",
       " ('3', 'consideraciones'),\n",
       " ('4', 'conclusiones'),\n",
       " ('5385', 'petitorio'),\n",
       " ('medico', 'legales')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dfLimpio.apply(remove_stops)\n",
    "\n",
    "tokens = nltk.wordpunct_tokenize(str(text))\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "sorted(finder.nbest(bi_gram.raw_freq,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import ieer\n",
    "docs = ieer.parsed_docs('NYT_19980315')\n",
    "tree = docs[1].text\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sem import relextract\n",
    "pairs = relextract.tree2semi_rel(dfLimpio)\n",
    "for s, tree in pairs[18:22]:\n",
    "    print('(\"...%s\", %s)' % (\" \".join(s[-5:]),tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = nltk.word_tokenize(str(dfLimpio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag  = nltk.pos_tag(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 'CD'),\n",
       " ('proemio', 'JJ'),\n",
       " ('1', 'CD'),\n",
       " ('antecedentes', 'NNS'),\n",
       " ('interes', 'NNS'),\n",
       " ('medico', 'VBP'),\n",
       " ('legal', 'JJ'),\n",
       " ('2', 'CD'),\n",
       " ('examen', 'NNS'),\n",
       " ('fisico', 'JJ'),\n",
       " ('actor', 'NN'),\n",
       " ('3', 'CD'),\n",
       " ('consideraciones', 'NNS'),\n",
       " ('medico', 'VBD'),\n",
       " ('legales', 'NNS'),\n",
       " ('4', 'CD'),\n",
       " ('conclusiones', 'NNS'),\n",
       " ('...', ':'),\n",
       " ('5385', 'CD'),\n",
       " ('petitorio', 'NN'),\n",
       " ('5386', 'CD'),\n",
       " ('historia', 'NN'),\n",
       " ('clinica', 'NN'),\n",
       " ('5387', 'CD'),\n",
       " ('estudios', 'NNS'),\n",
       " ('solicitados', 'JJ'),\n",
       " ('5388', 'CD'),\n",
       " ('puntos', 'NN'),\n",
       " ('periciales', 'NNS'),\n",
       " ('5389', 'CD'),\n",
       " ('consideraciones', 'NNS'),\n",
       " ('conclusiones', 'NNS'),\n",
       " ('medico', 'VBP'),\n",
       " ('legales', 'NNS'),\n",
       " ('Name', 'NN'),\n",
       " (':', ':'),\n",
       " ('titulo', 'NN'),\n",
       " (',', ','),\n",
       " ('Length', 'NNP'),\n",
       " (':', ':'),\n",
       " ('5390', 'CD'),\n",
       " (',', ','),\n",
       " ('dtype', 'NN'),\n",
       " (':', ':'),\n",
       " ('object', 'NN')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('proyecto-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "009f8d941f1a5e5fa5c53bd1fe1f1a27e75894b84557fa679569ec42254b5e02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
